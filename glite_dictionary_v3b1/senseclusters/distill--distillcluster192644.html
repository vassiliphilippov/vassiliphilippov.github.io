<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="../style.css">
  <title>distill · To transfer knowledge</title>
</head>
<body>
  <div class="container">
<header>
  
    <p class='small'><a href='../headwords/distill.html'>&larr; Back to distill</a></p>
  
  <h1>distill</h1>
  <div class='part-of-speech non-italic'>verb</div>
  <div class='headword-subtitle' style='font-size: 1.3em; margin-top: 0.5rem;'>transfer knowledge between ML models</div>
</header>
<main>
  <section class="cluster-card">
    
      
      <div style="float:right;">
    <div class="diff-pills-container">
      
      
      
      
      
      
      
      
      
        
      
      <span class="diff-pill" style="--diff-color: #ef4444" title="Main difficulty (based on prediction)">7.4</span>
      
      
      
    </div>
  </div>
    
    <div class='headword-pronunciation'>Pronunciation: / dɪˈstɪl / (AmE), / dɪˈstɪl / (BrE)</div>
    <div class="definition">(in machine learning) to transfer learned knowledge or behaviour from a larger, typically slower model (the &#34;teacher&#34;) to a smaller, faster model (the &#34;student&#34;), usually by training the smaller model to match the larger model&#39;s outputs or internal representations.</div>
    
      <div class="examples"><h3>Examples</h3>
        <blockquote>To <b>distill</b> a model</blockquote><blockquote>Researchers <b>distil</b> the teacher model into a compact student network for deployment on mobile devices.</blockquote><blockquote>To improve speed, the team <b>distilled</b> the large language model into a smaller, efficient variant.</blockquote><blockquote>Their paper shows how to <b>distil</b> knowledge using soft targets and intermediate feature matching techniques.</blockquote>
      </div>
    
    
      <div class='related'><h3>Related Terms</h3>
        <ul>
          
            <li><span class='muted'>synonym</span>: <a href='../headwords/knowledge_distillation-7e52da4c90.html'>knowledge distillation</a> - <em>the machine-learning technique of transferring knowledge from a large model to a smaller one</em></li>
          
            <li><span class='muted'>synonym</span>: <a href='../headwords/model_compression-9cc910c9f0.html'>model compression</a> - <em>methods for producing smaller, more efficient machine-learning models from larger ones</em></li>
          
        </ul>
      </div>
    
    
      <div><h3>Senses in This Cluster</h3>
        <ul>
          
            <li><code>distil|augmented.1255</code> - transfer knowledge between ML models</li>
          
        </ul>
      </div>
    
    
      <div><h3>Surface Forms</h3>
        <ul>
          
            <li><a href='../surfaceforms/distill.html'><code>distill</code></a> - infinitive (base)</li>
          
            <li><a href='../surfaceforms/distills.html'><code>distills</code></a> - third_person_singular</li>
          
            <li><a href='../surfaceforms/distilling.html'><code>distilling</code></a> - present_participle</li>
          
            <li><a href='../surfaceforms/distilled.html'><code>distilled</code></a> - past</li>
          
            <li><a href='../surfaceforms/distilled.html'><code>distilled</code></a> - past_participle</li>
          
            <li><a href='../surfaceforms/distil.html'><code>distil</code></a> - infinitive [british]</li>
          
            <li><a href='../surfaceforms/distils.html'><code>distils</code></a> - third_person_singular [british]</li>
          
            <li><a href='../surfaceforms/distil.html'><code>distil</code></a> - third_person_singular [british, common_misspelling]</li>
          
        </ul>
      </div>
    
    
    
      <div><h3>Origin Information</h3>
        <div class='origin-tree'>
          
            <div class='origin-node'>
              <strong>Transform</strong>
              <div class='origin-details'>
                <div><span class='muted'>Step:</span> regional_clusters_merged</div>
                <div><span class='muted'>Result ID:</span> <code>distill|cluster.192644</code></div>
                <div><span class='muted'>Description:</span> (in machine learning) to transfer learned knowledge or behaviour from a larger, typically slower model (the &#34;teacher&#34;) to a smaller, faster model (the &#34;student&#34;), usually by training the smaller model to match the larger model&#39;s outputs or internal representations.</div>
              </div>
              
                <div class='origin-child'>
                  <div class='muted' style='margin: 0.5rem 0;'>↓ From:</div>
                  <div class='origin-nested'>{
  "description": "(in machine learning) to transfer learned knowledge or behaviour from a larger, typically slower model (the \"teacher\") to a smaller, faster model (the \"student\"), usually by training the smaller model to match the larger model\u0027s outputs or internal representations.",
  "headword": "distil",
  "kind": "TRANSFORM",
  "result_id": "distil|cluster.89837",
  "source_origin": {
    "creation_step": "new_senses_filtered",
    "description": "(in machine learning) to transfer learned knowledge or behaviour from a larger, typically slower model (the \"teacher\") to a smaller, faster model (the \"student\"), usually by training the smaller model to match the larger model\u0027s outputs or internal representations.",
    "headword": "distil",
    "kind": "NEW",
    "reason": "New sense found for existing headword",
    "sense_id": "distil|augmented.1255"
  },
  "transformation_step": "clustered_senses"
}</div>
                </div>
              
            </div>
          
        </div>
      </div>
    
    
      <div class='muted' style='font-size: 0.9em; margin-top: 10px;'>
        <div><span>Cluster ID:</span> <code>distill|cluster.192644</code></div>
        
        
          <div><span>V3 Concept ID:</span> <code>ct:ct7eUbmMDZFfsscfDyy6Xdistill</code></div>
        
      </div>
    
  </section>
</main>
  </div>
</body>
</html>
