<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="../style.css">
  <title>memorization · Overfitting data</title>
</head>
<body>
  <div class="container">
<header>
  
    <p class='small'><a href='../headwords/memorization.html'>&larr; Back to memorization</a></p>
  
  <h1>memorization</h1>
  <div class='part-of-speech non-italic'>noun</div>
  <div class='headword-subtitle' style='font-size: 1.3em; margin-top: 0.5rem;'>model overfitting / remembering training data</div>
</header>
<main>
  <section class="cluster-card">
    
      
      <div style="float:right;">
    <div class="diff-pills-container">
      
      
      
      
      
      
      
      
      
        
      
      <span class="diff-pill" style="--diff-color: #f59e0b" title="Main difficulty (based on prediction)">6.1</span>
      
      
      
    </div>
  </div>
    
    <div class='headword-pronunciation'>Pronunciation: /me.mə.rə.ˈzeɪ.ʃən/ /; / /me.mə.rɪ.ˈzeɪ.ʃən/ (AmE), /me.mə.rə.ˈzeɪ.ʃən/ /; / /me.mə.rɪ.ˈzeɪ.ʃən/ (BrE)</div>
    <div class="definition">(in machine learning) the tendency of a model to &#39;remember&#39; or fit its training data too closely, storing idiosyncratic examples so it performs poorly on new, unseen data (see overfitting).</div>
    
      <div class="examples"><h3>Examples</h3>
        <blockquote>Model <b>memorization</b></blockquote><blockquote>The research team discovered the model's <b>memorization</b> of training images caused poor performance on new datasets.</blockquote><blockquote>Regularization techniques are used to reduce <b>memorization</b> and improve a neural network's ability to generalize.</blockquote><blockquote>They measured <b>memorization</b> by comparing training accuracy with test accuracy across multiple epochs.</blockquote><blockquote>Researchers warned that the model's <b>memorisation</b> of training examples could leak private user information.</blockquote><blockquote>To avoid <b>memorisation</b>, the team added dropout and data augmentation during model training.</blockquote><blockquote>Journal reviewers criticised the paper for evidence of <b>memorisation</b> rather than genuine generalisation to new data.</blockquote>
      </div>
    
    
      <div class='related'><h3>Related Terms</h3>
        <ul>
          
            <li><span class='muted'>antonym</span>: <a href='../headwords/generalisation.html'>generalisation</a> - <em>the ability of a model to perform well on unseen data; opposite of memorisation in ML contexts</em></li>
          
            <li><span class='muted'>antonym</span>: <a href='../headwords/generalization.html'>generalization</a> - <em>the ability of a model to perform well on new, unseen data</em></li>
          
            <li><span class='muted'>synonym</span>: <a href='../headwords/overfitting.html'>overfitting</a> - <em>a model fitting training data so closely that it fails to generalize to new data</em></li>
          
        </ul>
      </div>
    
    
      <div><h3>Senses in This Cluster</h3>
        <ul>
          
            <li><code>memorizati|augmented.2182</code> - model overfitting / remembering training data</li>
          
            <li><code>memorisati|augmented.2178</code> - model storing training data</li>
          
        </ul>
      </div>
    
    
      <div><h3>Surface Forms</h3>
        <ul>
          
            <li><a href='../surfaceforms/memorization.html'><code>memorization</code></a> - singular (base)</li>
          
            <li><a href='../surfaceforms/memorizations.html'><code>memorizations</code></a> - plural</li>
          
            <li><a href='../surfaceforms/memorisation.html'><code>memorisation</code></a> - singular [british]</li>
          
            <li><a href='../surfaceforms/memorisationg.html'><code>memorisationg</code></a> - singular [british, common_misspelling]</li>
          
            <li><a href='../surfaceforms/memorisations.html'><code>memorisations</code></a> - plural [british]</li>
          
        </ul>
      </div>
    
    
    
      <div><h3>Origin Information</h3>
        <div class='origin-tree'>
          
            <div class='origin-node'>
              <strong>Merge</strong>
              <div class='origin-details'>
                <div><span class='muted'>Step:</span> regional_clusters_merged</div>
                <div><span class='muted'>Result ID:</span> <code>memorizati|cluster.193622</code></div>
                <div><span class='muted'>Reason:</span> Regional spelling variants merged into single cluster</div>
                <div><span class='muted'>Description:</span> (in machine learning) the tendency of a model to &#39;remember&#39; or fit its training data too closely, storing idiosyncratic examples so it performs poorly on new, unseen data (see overfitting).</div>
              </div>
              
                <div class='origin-child'>
                  <div class='muted' style='margin: 0.5rem 0;'>↓ From 2 sources:</div>
                  <div class='origin-nested'>[
  {
    "description": "(in machine learning) the tendency of a model to reproduce or store exact training examples or data fragments instead of learning general patterns, often causing poor performance on new data or data leakage.",
    "headword": "memorisation",
    "kind": "TRANSFORM",
    "result_id": "memorisati|cluster.29916",
    "source_origin": {
      "creation_step": "new_senses_filtered",
      "description": "(in machine learning) the tendency of a model to reproduce or store exact training examples or data fragments instead of learning general patterns, often causing poor performance on new data or data leakage.",
      "headword": "memorisation",
      "kind": "NEW",
      "reason": "New sense found for existing headword",
      "sense_id": "memorisati|augmented.2178"
    },
    "transformation_step": "clustered_senses"
  },
  {
    "description": "(in machine learning) the tendency of a model to \u0027remember\u0027 or fit its training data too closely, storing idiosyncratic examples so it performs poorly on new, unseen data (see overfitting).",
    "headword": "memorization",
    "kind": "TRANSFORM",
    "result_id": "memorizati|cluster.30385",
    "source_origin": {
      "creation_step": "new_senses_filtered",
      "description": "(in machine learning) the tendency of a model to \u0027remember\u0027 or fit its training data too closely, storing idiosyncratic examples so it performs poorly on new, unseen data (see overfitting).",
      "headword": "memorization",
      "kind": "NEW",
      "reason": "New sense found for existing headword",
      "sense_id": "memorizati|augmented.2182"
    },
    "transformation_step": "clustered_senses"
  }
]</div>
                </div>
              
            </div>
          
        </div>
      </div>
    
    
      <div class='muted' style='font-size: 0.9em; margin-top: 10px;'>
        <div><span>Cluster ID:</span> <code>memorizati|cluster.193622</code></div>
        
        
          <div><span>V3 Concept ID:</span> <code>ct:ct7fRLymU9oQWUxRc2LVUmemoriza</code></div>
        
      </div>
    
  </section>
</main>
  </div>
</body>
</html>
