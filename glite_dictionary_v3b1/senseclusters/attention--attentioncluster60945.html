<!doctype html><html lang=en><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><link href="../style.css?v=104" rel=stylesheet><link href=../youtube_player.css rel=stylesheet><script src=../youtube_player.js></script><title>attention ¬∑ A way a computer model focuses on the most important parts of input</title><body><div class=container><header><p class=small><a href=../headwords/attention.html>‚Üê Back to attention</a> | <a href=../index.html>Home</a> | <a href=../search.html>Search</a> | <a href=../random.html>Random</a></header><main><section class=headword-section-no-card><div class=headword-header><div class=headword-title-row><h1 class=headword-title>attention</h1><button aria-label="Play pronunciation" onclick="playAudio('https://cdn.glite.ai/pa/cfac1bab-4c01-46b7-b8db-819768907cbf.mp3')" class=headword-audio-btn type=button>üîà</button></div><div class=headword-metrics><div class=diff-freq-container><span class=freq-badge>0.3/m</span><span class=diff-pill style=--diff-color:#ef4444>7.2</span></div></div></div><div class=headword-ui-definition>A way a computer model focuses on the most important parts of input</div><div class=headword-meta-row><span class=meta-text>noun</span><span class="meta-text ipa-text"> <span class=meta-label>US</span> <span class=ipa-value>/ 59c8t5bn8359n /</span> </span></div><audio id=pronunciation-audio preload=none><source src=https://cdn.glite.ai/pa/cfac1bab-4c01-46b7-b8db-819768907cbf.mp3 type=audio/mpeg></audio><script>function playAudio(url){let audio=document.getElementById(`pronunciation-audio`);if(audio)try{audio.src=url,audio.play()}catch(err){console.error(`Unable to play audio`,err)}}</script></section><section class="cluster-card examples-card"><h3>Examples</h3><div class=examples-list data-example-list><blockquote><b>Attention</b> mechanism in AI</blockquote><blockquote>The transformer model uses an <b>attention</b> mechanism to weigh each word in the input sequence.</blockquote><blockquote>Researchers improved translation quality by adding multi-head <b>attention</b> to the neural network architecture.</blockquote><blockquote class=example-hidden data-hidden-example=true>By visualising <b>attention</b> weights, they could see which tokens the model focused on during prediction.</blockquote><blockquote class=example-hidden data-hidden-example=true>The model requires constant <b>attention</b> to improve accuracy.</blockquote><blockquote class=example-hidden data-hidden-example=true>The neural network needs more <b>attention</b> to detail.</blockquote><blockquote class=example-hidden data-hidden-example=true>In machine learning, <b>attention</b> mechanisms help models focus.</blockquote><blockquote class=example-hidden data-hidden-example=true>The <b>attention</b> given to the training data is crucial.</blockquote><blockquote class=example-hidden data-hidden-example=true>The algorithm's <b>attention</b> to relevant data improves performance.</blockquote><blockquote class=example-hidden data-hidden-example=true>The model's <b>attention</b> can be adjusted dynamically.</blockquote><blockquote class=example-hidden data-hidden-example=true>Look at how the model prioritizes <b>attention</b>!</blockquote><blockquote class=example-hidden data-hidden-example=true>What kind of <b>attention</b> does this model require?</blockquote></div><button data-hide-text="Hide extra examples" data-show-text="Show 9 more" class=examples-toggle data-examples-toggle type=button>Show 9 more</button></section><section class=youtube-examples-section-no-card><h3>Video Examples</h3><div id=youtube-player-attention|cluster.60945></div><script>(function(){let examplesData=[{channel_name:`MIT OpenCourseWare`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:1658590,phrase:`What I'd like to do now is turn our <b>attention</b> to discrete time filters.`,start_ms:1652950,video_id:`P5Ce9tbK86M`,video_title:`Lecture 12, Filtering | MIT RES.6.007 Signals and Systems, Spring 2011`},{channel_name:`Sabine Hossenfelder`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:392080,phrase:`These assign a higher weight or increased <b>attention</b> to words that are more important.`,start_ms:386080,video_id:`CSTfgYynziw`,video_title:`How could we tell whether AI has become conscious?`},{channel_name:`GaryVee`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:1090120,phrase:`And that attention, <b>attention</b>, my friends, is the asset.`,start_ms:1085840,video_id:`yTNX1sXeMdc`,video_title:`Internet Week - Keynote 2015 - Gary Vaynerchuk`},{channel_name:`Robinson Erhardt`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:5247480,phrase:`Their title was <b>attention</b> is all you need.`,start_ms:5243480,video_id:`0iZ8-SxrtZI`,video_title:`Jay McClelland: Deep Learning, Neural Networks, & Artificial Intelligence | Robinson's Podcast #124`},{channel_name:`GaryVee`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:26560,phrase:`So the science around the art <b>attention</b> is the number one asset.`,start_ms:21520,video_id:`O8341NsMf9M`,video_title:`The Opportunity For Every Business In 2024`},{channel_name:`Lex Fridman`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:4595680,phrase:`And they also do, they do joint <b>attention</b> really well with gays.`,start_ms:4589040,video_id:`qwsft6tmvBA`,video_title:`Lisa Feldman Barrett: How the Brain Creates Emotions |  MIT Artificial General Intelligence (AGI)`},{channel_name:`Robinson Erhardt`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:3063e3,phrase:`It means that really <b>attention</b> means signal enhancement.`,start_ms:3057280,video_id:`hDe4DsoHuIQ`,video_title:`Michael Graziano: The Attention Schema Theory of Consciousness | Robinson's Podcast #169`},{channel_name:`Stanford`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:1289880,phrase:`Bring your <b>attention</b> to the chest and the ribs.`,start_ms:1285760,video_id:`jBWsI_Rz50A`,video_title:`Guided Meditation with Swami Vidyadhishananda - Contemplation By Design Summit 2018`},{channel_name:`Channel UCIfVwP1uAsfwEtWr9jw8xOg`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:225840,phrase:`So Spear turned his <b>attention</b> to making the city bomb proof.`,start_ms:221680,video_id:`uGEuBhvSubM`,video_title:`Hitler's Supercity Part 4`},{channel_name:`Stanford`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:75120,phrase:`And Ed did a lot of research on managing human <b>attention</b> and search.`,start_ms:68600,video_id:`0sBdQO2Xd6o`,video_title:`Natural Interactions & Computing for Global Development`},{channel_name:`Chris Williamson`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:4309600,phrase:`And you know, conflict is <b>attention</b> and attention is currency.`,start_ms:4304320,video_id:`tQpZSVCTZCo`,video_title:`Legacy Media Is Lying To You - Balaji Srinivasan`},{channel_name:`etthehiphoppreacher`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:5889600,phrase:`So do me a favor, send it to <b>attention</b> to school days.`,start_ms:5885440,video_id:`J5mGtVNJ8I4`,video_title:`Eps. 93 - Love Conquers`},{channel_name:`etthehiphoppreacher`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:6349120,phrase:`So do me a favor, send it to <b>attention</b> to school days.`,start_ms:6344960,video_id:`v2z3d8s6ihg`,video_title:`Eps. 92 - A PHD in Addiction`},{channel_name:`Chris Williamson`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:889920,phrase:`We call this rote <b>attention</b> or rote activity.`,start_ms:886280,video_id:`0PwILi2fCSo`,video_title:`How To Regain Control Of Your Attention - Dr Gloria Mark`},{channel_name:`Film Courage`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:253440,phrase:`I don't know whether it was more of an <b>attention</b> grabbing technique in the beginning.`,start_ms:247e3,video_id:`rwFuRF2EUUk`,video_title:`Does Heritage Of Legacy Or Lineage Create A Stronger Sense Of Self Or Purpose? by Kalpana Pandit`},{channel_name:`Film Courage`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:253440,phrase:`I don't know whether it was more of an <b>attention</b> grabbing technique in the beginning.`,start_ms:247040,video_id:`0VSAg-gfHNk`,video_title:`Let's Be Fluid As Water & Live Our Dreams by Kalpana Pandit of Sulige Sikkidaaga`},{channel_name:`ReasonTV`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:803560,phrase:`Facebook sells your <b>attention</b> to advertisers, right?`,start_ms:799880,video_id:`aqdYbwY9vPU`,video_title:`Super Hacker George Hotz: I Can Make Your Car Drive Itself for Under $1,000`},{channel_name:`MIT OpenCourseWare`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:3127440,phrase:`And a lot of computer science <b>attention</b> has gone on to that.`,start_ms:3123280,video_id:`S6dw885-SZI`,video_title:`Lec 13 | MIT 18.086 Mathematical Methods for Engineers II`},{channel_name:`Lex Fridman`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:2050600,phrase:`And so there's different schools of thought on training <b>attention</b>, for instance.`,start_ms:2044200,video_id:`4iuepdI3wCU`,video_title:`Charan Ranganath: Human Memory, Imagination, Deja Vu, and False Memories | Lex Fridman Podcast #430`},{channel_name:`Rick Beato`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:836240,phrase:`But again, certain directors will have <b>attention</b> on this from a very early point.`,start_ms:830360,video_id:`WzAmjmUDuZw`,video_title:`The  Mark Isham Interview - Film Scoring and Solo Career`},{channel_name:`Andrew Huberman`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:4341680,phrase:`What we're about to talk about is when <b>attention</b> works and when attention falters.`,start_ms:4335880,video_id:`hFL6qRIJZ_Y`,video_title:`ADHD & How Anyone Can Improve Their Focus`},{channel_name:`Andrew Huberman`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:921040,phrase:`When you are in a state of elevated <b>attention</b> but very relaxed, guess what?`,start_ms:915080,video_id:`LRM5LutB538`,video_title:`LIVE EVENT Q&A: Dr. Andrew Huberman Question & Answer in Chicago, IL`},{channel_name:`Ear Biscuits`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:1147480,phrase:`We are constantly seeking the <b>attention</b> of a crowd as a for for a living.`,start_ms:1141800,video_id:`oRppKuE13Vs`,video_title:`What Our Body Language Says About Us | Ear Biscuits`},{channel_name:`Triggernometry`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:2205760,phrase:`But at the moment, of course, all <b>attention</b> is focused on the pandemic.`,start_ms:2199920,video_id:`UQWhTwFKiB4`,video_title:`Lord Nigel Lawson: "I've Never Been More Worried About This Country"`},{channel_name:`Mel Robbins`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:8728240,phrase:`It was an <b>attention</b> and focus issue.`,start_ms:8724840,video_id:`z4M8aMsrzb8`,video_title:`How to BEAT Your ANXIETY and Become Mentally Strong | Mel Robbins`},{channel_name:`Manny Mua`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:113720,phrase:`But anyway, let's go ahead and get into this testing this Jelly eyebrow <b>attention</b> Jelly.`,start_ms:107400,video_id:`bLNEjiMEcxw`,video_title:`VIRAL JELLY EYEBROW EXTENSIONS TESTED! WTF!`},{channel_name:`The Knowledge Project Podcast`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:801160,phrase:`So that it's a, it's an embodied <b>attention</b> as well as the mental perspective.`,start_ms:794240,video_id:`D9ayv-y4XBo`,video_title:`A Practical Guide on Finding Inner Peace | Jack Kornfield | Knowledge Project Podcast 156`},{channel_name:`Triggernometry`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:1525920,phrase:`Their their job is to make money <b>attention</b> eyeballs.`,start_ms:1522480,video_id:`VM_LRw0957w`,video_title:`Zuby: "This is a Moral Panic"`},{channel_name:`Dr. Phil`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:4457480,phrase:`Just that energy that you get from that <b>attention</b> is, is measurable and it's addictive.`,start_ms:4451200,video_id:`MfHWJaNJC74`,video_title:`Phil In The Blanks #21 - Ron White`},{channel_name:`Big Think`,concept_id:`ct:ct7ez7LcyPwuTyfoB2k6pattentio`,end_ms:179360,phrase:`They got into this notion that they would manipulate <b>attention</b> first with rewards to create habits.`,start_ms:172520,video_id:`C2Ag1iQKWeM`,video_title:`Data spies: The dark and shady practices of Silicon Valley | Roger McNamee | Big Think`}],initPlayer=()=>{new YouTubeExamplesPlayer(`youtube-player-attention|cluster.60945`,examplesData).initialize()};document.readyState===`loading`?document.addEventListener(`DOMContentLoaded`,initPlayer,{once:!0}):initPlayer()})();</script></section><section class="cluster-card power-grades-card"><h3>Power Grade</h3><div class=power-grade-layout><div class=power-grade-gradient></div><div class=power-grade-list><div class=power-grade-row><div class=power-grade-arrow>‚Üë</div><div class=power-grade-label><a href=../senseclusters/attention--attentioncluster60945.html>attention</a><span aria-label="Current selection" class=power-grade-current-dot></span></div><div class=power-grade-pill><div class=diff-freq-container><span class=freq-badge>0.3/m</span><span class=diff-pill style=--diff-color:#ef4444>7.2</span></div></div></div><div class=power-grade-row><div class=power-grade-arrow></div><div class=power-grade-label><a href=../senseclusters/unweighting--unweightincluster58897.html>unweighting</a></div><div class=power-grade-pill><div class=diff-freq-container><span class=freq-badge>0.00/m</span><span class=diff-pill style=--diff-color:#ef4444>7.9</span></div></div></div></div></div></section><section class="cluster-card forms-card"><h3>Surface Forms</h3><ul><li><a href=../surfaceforms/attention.html><code>attention</code></a> - singular (base)<li><a href=../surfaceforms/attentions.html><code>attentions</code></a> - plural</ul></section><div class=technical-details-container><button class=show-technical-details-btn onclick=toggleTechnicalDetails() type=button>Show Technical Details</button><div class=technical-details-content id=technical-details style=display:none><section class="cluster-card related-card"><h3>Related Terms</h3><ul><li><span class=muted>synonym</span>: <a href=../headwords/attention_mechanism-a4e5555159.html>attention mechanism</a> - <em>a neural network component that computes weights over input elements to emphasize relevant information</em><li><span class=muted>synonym</span>: <a href=../headwords/self-attention.html>self-attention</a> - <em>a form of attention where a sequence's elements attend to other elements in the same sequence</em></ul></section><section class="cluster-card senses-card"><h3>Senses in This Cluster</h3><ul><li><code>attention|augmented.556</code> - ML mechanism that weights input elements</ul></section><section class="cluster-card frequency-card"><h3>Frequency Details</h3><div class=frequency-details><table style=border-collapse:collapse;width:100%;margin-bottom:1rem><thead><tr style="border-bottom:2px solid #ddd"><th style=text-align:left;padding:8px>Corpus<th style=text-align:right;padding:8px>Occurrences<th style=text-align:right;padding:8px>Total Words<th style=text-align:right;padding:8px>Per Million<tbody><tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>mag</code><td style=text-align:right;padding:8px>33<td style=text-align:right;padding:8px>67,696,835<td style=text-align:right;padding:8px>0.49<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>web</code><td style=text-align:right;padding:8px>54<td style=text-align:right;padding:8px>98,085,816<td style=text-align:right;padding:8px>0.55<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>spok</code><td style=text-align:right;padding:8px>16<td style=text-align:right;padding:8px>98,046,614<td style=text-align:right;padding:8px>0.16<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>blog</code><td style=text-align:right;padding:8px>68<td style=text-align:right;padding:8px>96,701,264<td style=text-align:right;padding:8px>0.70<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>news</code><td style=text-align:right;padding:8px>13<td style=text-align:right;padding:8px>95,124,381<td style=text-align:right;padding:8px>0.14<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>fic</code><td style=text-align:right;padding:8px>19<td style=text-align:right;padding:8px>93,827,527<td style=text-align:right;padding:8px>0.20<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>acad</code><td style=text-align:right;padding:8px>117<td style=text-align:right;padding:8px>30,866,226<td style=text-align:right;padding:8px>3.79<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>tvm</code><td style=text-align:right;padding:8px>12<td style=text-align:right;padding:8px>91,324,286<td style=text-align:right;padding:8px>0.13<tr style="border-top:2px solid #ddd;font-weight:700"><td style=padding:8px>Mean<td style=text-align:right;padding:8px>‚Äî<td style=text-align:right;padding:8px>‚Äî<td style=text-align:right;padding:8px>0.77<tr style=font-weight:700><td style=padding:8px>Median<td style=text-align:right;padding:8px>‚Äî<td style=text-align:right;padding:8px>‚Äî<td style=text-align:right;padding:8px>0.34</table><div style=background-color:#f8f9fa;border-radius:4px;margin-top:1rem;padding:12px><div style=margin-bottom:8px;font-weight:700>UI Frequency (0.23 per million)</div><div style=margin-left:1rem><div><strong>Per year:</strong> 4.1 occurrences</div><div><strong>Per month:</strong> 0.3 occurrences</div><div><strong>Per week:</strong> 0.1 occurrences</div></div></div></div></section><section class="cluster-card origin-section"><h3>Origin Information</h3><div class=origin-tree><div class=origin-card><div class="origin-header origin-transform"><span class=origin-icon>üîÑ</span><span class=origin-type>Transform</span></div><div class=origin-body><div class=origin-field><span class=origin-label>Step:</span><span class=origin-value>regional_clusters_merged</span></div><div class=origin-field><span class=origin-label>Result ID:</span><code class=origin-code>attention|cluster.60945</code></div><div class="origin-field origin-description">(in machine learning) a mechanism in neural networks that assigns dynamic weights to different parts of the input so the model focuses on the most relevant information.</div></div><div class=origin-arrow>‚Üì</div><div class="origin-card origin-nested-card"><div class="origin-header origin-transform"><span class=origin-icon>üîÑ</span><span class=origin-type>Transform</span></div><div class=origin-body><div class=origin-field><span class=origin-label>Step:</span><span class=origin-value>clustered_senses</span></div><div class=origin-field><span class=origin-label>Result ID:</span><code class=origin-code>attention|cluster.60945</code></div><div class="origin-field origin-description">(in machine learning) a mechanism in neural networks that assigns dynamic weights to different parts of the input so the model focuses on the most relevant information.</div></div><div class=origin-arrow>‚Üì</div><div class="origin-card origin-nested-card"><div class="origin-header origin-new"><span class=origin-icon>‚ú®</span><span class=origin-type>New Sense</span></div><div class=origin-body><div class=origin-field><span class=origin-label>Created in:</span><span class=origin-value>new_senses_filtered</span></div><div class=origin-field><span class=origin-label>Reason:</span><span class=origin-value>New sense found for existing headword</span></div><div class="origin-field origin-description">(in machine learning) a mechanism in neural networks that assigns dynamic weights to different parts of the input so the model focuses on the most relevant information.</div></div></div></div></div></div></section><section class="cluster-card meta-card"><h3>Details</h3><div class="muted meta-grid"><div><span>Cluster ID:</span><code>attention|cluster.60945</code></div><div><span>V3 Concept ID:</span><code>ct:ct7ez7LcyPwuTyfoB2k6pattentio</code></div></div></section><section class="cluster-card other-fields-card"><h3>Other Fields</h3><div class="muted meta-grid"><div><span>Description:</span> (in machine learning) a mechanism in neural networks that assigns dynamic weights to different parts of the input so the model focuses on the most relevant information.</div></div></section></div></div><script>function toggleTechnicalDetails(){let content=document.getElementById(`technical-details`),btn=document.querySelector(`.show-technical-details-btn`);content.style.display===`none`?(content.style.display=`block`,btn.textContent=`Hide Technical Details`):(content.style.display=`none`,btn.textContent=`Show Technical Details`)}</script></main></div><script>(function(){let list=document.querySelector(`[data-example-list]`),toggleBtn=document.querySelector(`[data-examples-toggle]`);if(!list||!toggleBtn)return;let hiddenItems=list.querySelectorAll(`[data-hidden-example="true"]`),hide=()=>hiddenItems.forEach(item=>item.classList.add(`example-hidden`));hide(),toggleBtn.addEventListener(`click`,()=>{toggleBtn.getAttribute(`data-expanded`)===`true`?(hide(),toggleBtn.setAttribute(`data-expanded`,`false`),toggleBtn.textContent=toggleBtn.dataset.showText):(hiddenItems.forEach(item=>item.classList.remove(`example-hidden`)),toggleBtn.setAttribute(`data-expanded`,`true`),toggleBtn.textContent=toggleBtn.dataset.hideText)})})();</script>