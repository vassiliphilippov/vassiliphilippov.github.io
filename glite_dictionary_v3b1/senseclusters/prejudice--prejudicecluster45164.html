<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="../style.css">
  <title>prejudice · Unfair computer bias</title>
</head>
<body>
  <div class="container">
<header>
  
    <p class='small'><a href='../headwords/prejudice.html'>&larr; Back to prejudice</a></p>
  
  <h1>prejudice</h1>
  <div class='part-of-speech non-italic'>noun</div>
  <div class='headword-subtitle' style='font-size: 1.3em; margin-top: 0.5rem;'>bias in automated systems</div>
</header>
<main>
  <section class="cluster-card">
    <div class='headword-pronunciation'>Pronunciation: / c8pr5bd9259d6as / (AmE), / c8pr5bd928ad6as / (BrE)</div>
    <div class="definition">(in computing and data science) unfair or discriminatory bias that is embedded in or produced by algorithms, models, or datasets, causing unequal outcomes for particular groups.</div>
    
      <div class="examples"><h3>Examples</h3>
        <blockquote>Algorithmic <b>prejudice</b></blockquote><blockquote>Researchers discovered that the hiring algorithm introduced racial <b>prejudice</b> into candidate rankings.</blockquote><blockquote>Regulators asked the company to audit its system for signs of <b>prejudice</b> against low-income applicants.</blockquote><blockquote>Civil-rights groups warned that unexamined training data could recreate historical <b>prejudice</b> in automated decisions.</blockquote>
      </div>
    
    
      <div class='related'><h3>Related Terms</h3>
        <ul>
          
            <li><span class='muted'>synonym</span>: <a href='../headwords/algorithmic_bias-17a6e780fb.html'>algorithmic bias</a> - <em>systematic and repeatable errors in a computer system that create unfair outcomes</em></li>
          
            <li><span class='muted'>synonym</span>: <a href='../headwords/bias.html'>bias</a> - <em>inclination or prejudice for or against one person or group; unfair preference or disadvantage</em></li>
          
            <li><span class='muted'>antonym</span>: <a href='../headwords/fairness.html'>fairness</a> - <em>the quality of treating people equally and without bias</em></li>
          
        </ul>
      </div>
    
    
      <div><h3>Senses in This Cluster</h3>
        <ul>
          
            <li><code>prejudice|augmented.2601</code> - bias in automated systems</li>
          
        </ul>
      </div>
    
    
      <div><h3>Surface Forms</h3>
        <ul>
          
            <li><a href='../surfaceforms/prejudice.html'><code>prejudice</code></a> - singular (base)</li>
          
            <li><a href='../surfaceforms/prejudices.html'><code>prejudices</code></a> - plural</li>
          
        </ul>
      </div>
    
    
    
      <div><h3>Origin Information</h3>
        <div class='origin-tree'>
          
            <div class='origin-node'>
              <strong>Transform</strong>
              <div class='origin-details'>
                <div><span class='muted'>Step:</span> regional_clusters_merged</div>
                <div><span class='muted'>Result ID:</span> <code>prejudice|cluster.45164</code></div>
                <div><span class='muted'>Description:</span> (in computing and data science) unfair or discriminatory bias that is embedded in or produced by algorithms, models, or datasets, causing unequal outcomes for particular groups.</div>
              </div>
              
                <div class='origin-child'>
                  <div class='muted' style='margin: 0.5rem 0;'>↓ From:</div>
                  <div class='origin-nested'>{
  "description": "(in computing and data science) unfair or discriminatory bias that is embedded in or produced by algorithms, models, or datasets, causing unequal outcomes for particular groups.",
  "headword": "prejudice",
  "kind": "TRANSFORM",
  "result_id": "prejudice|cluster.45164",
  "source_origin": {
    "creation_step": "new_senses_filtered",
    "description": "(in computing and data science) unfair or discriminatory bias that is embedded in or produced by algorithms, models, or datasets, causing unequal outcomes for particular groups.",
    "headword": "prejudice",
    "kind": "NEW",
    "reason": "New sense found for existing headword",
    "sense_id": "prejudice|augmented.2601"
  },
  "transformation_step": "clustered_senses"
}</div>
                </div>
              
            </div>
          
        </div>
      </div>
    
  </section>
</main>
  </div>
</body>
</html>
