<!doctype html><html lang=en><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><link href="../style.css?v=94" rel=stylesheet><link href=../youtube_player.css rel=stylesheet><script src=../youtube_player.js></script><title>entropy ¬∑ Uncertainty measure</title><body><div class=container><header><p class=small><a href=../headwords/entropy.html>‚Üê Back to entropy</a> | <a href=../index.html>Home</a> | <a href=../search.html>Search</a> | <a href=../random.html>Random</a><h1>entropy</h1><div class="part-of-speech non-italic">noun</div><div class=headword-subtitle style=margin-top:.5rem;font-size:1.3em>uncertainty measure in information theory</div></header><main><section class=cluster-card><div style=float:right><div class=diff-freq-container><span class=diff-pill style=--diff-color:#ef4444>9.0</span><span class=freq-badge>0.09/m</span></div></div><div class=headword-pronunciation><span>US</span><span onclick="playAudio('https://cdn.glite.ai/pa/7fc1a505-5f38-49de-ab36-a93f76e04b9a.mp3')" class=audio-icon style=cursor:pointer;margin-left:4px;margin-right:4px>üîà</span><span>/ Àà…õn.tr…ô.pi /</span><br><span>UK</span><span>/ Àà…õn.tr…ô.pi /</span></div><audio id=pronunciation-audio preload=none><source src=https://cdn.glite.ai/pa/7fc1a505-5f38-49de-ab36-a93f76e04b9a.mp3 type=audio/mpeg></audio><script>function playAudio(url){let audio=document.getElementById(`pronunciation-audio`);audio&&audio.play()}</script><div class=definition>How hard it is to predict which message will appear in a set</div><div class=examples><h3>Examples</h3><blockquote>Information <b>entropy</b></blockquote><blockquote>A random sequence of letters has higher <b>entropy</b> than a predictable phrase.</blockquote><blockquote>Shannon's concept of <b>entropy</b> revolutionized digital communication.</blockquote><blockquote>The <b>entropy</b> of the compressed file is significantly lower than that of the original data.</blockquote></div><div class=concept-extractor-examples><h3>Additional Examples</h3><blockquote>Higher <b>entropy</b> indicates more uncertainty.</blockquote><blockquote>Can you explain the concept of <b>entropy</b>?</blockquote><blockquote>The <b>entropies</b> of different systems can be compared.</blockquote><blockquote>In information theory, <b>entropy</b> quantifies uncertainty.</blockquote><blockquote>How does <b>entropy</b> relate to data compression?</blockquote><blockquote>The <b>entropy</b> value helps in predicting outcomes.</blockquote><blockquote>Researchers study the <b>entropies</b> of various algorithms.</blockquote><blockquote>The concept of <b>entropy</b> is crucial in statistics.</blockquote><blockquote>Did you know that <b>entropy</b> can measure disorder?</blockquote><blockquote>In a chaotic system, the <b>entropy</b> is very high.</blockquote><blockquote>The <b>entropy</b> of a perfect signal is zero.</blockquote><blockquote>Entropy varies among different types of <b>messages</b>.</blockquote><blockquote>The study of <b>entropy</b> is fascinating!</blockquote><blockquote>In communication, <b>entropy</b> helps assess message efficiency.</blockquote><blockquote>What factors influence the <b>entropy</b> of a dataset?</blockquote><blockquote>The <b>entropy</b> calculation can be complex.</blockquote><blockquote>The <b>entropies</b> of different signals were analyzed.</blockquote><blockquote>The <b>entropy</b> of a message affects its clarity.</blockquote><blockquote>I find <b>entropy</b> quite intriguing!</blockquote><blockquote>Understanding <b>entropy</b> is essential for coding.</blockquote></div><div class=youtube-examples-section><h3>Video Examples</h3><div id=youtube-player-entropy|cluster.135947></div><script>(function(){let examplesData=[{channel_name:`Stanford`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:1222600,phrase:`Let's try to guess what the <b>entropy</b> of a vibrating string is.`,start_ms:1218560,video_id:`5p4PvLUHH88`,video_title:`Lecture 6 | Topics in String Theory`},{channel_name:`Robinson Erhardt`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:2919720,phrase:`And the <b>entropy</b> is in turn the log of the number of of possible microstates.`,start_ms:2913e3,video_id:`nuJaZihtfn4`,video_title:`Tony Padilla: Mathematical Platonism, Intergalactic Doppelg√§ngers, and Gigantic Numbers | RP#155`},{channel_name:`MIT OpenCourseWare`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:1776760,phrase:`OK, So for example, the <b>entropy</b> of the motif of.`,start_ms:1770520,video_id:`1EMonM7qAU8`,video_title:`9. Modeling and Discovery of Sequence Motifs`},{channel_name:`MIT OpenCourseWare`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:2487880,phrase:`What's the magic word <b>entropy</b> of mixing, right, right.`,start_ms:2483280,video_id:`Bd7PVX7rohQ`,video_title:`Lec 19 | MIT 5.60 Thermodynamics & Kinetics, Spring 2008`},{channel_name:`Stanford`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:3416640,phrase:`It's connected with entropy, but <b>entropy</b> is a is a subtle thing.`,start_ms:3410520,video_id:`H1Zbp6__uNw`,video_title:`Lecture 1 | Modern Physics: Statistical Mechanics`},{channel_name:`StarTalk`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:1243640,phrase:`That's that's the coolest explanation of <b>entropy</b> I've ever heard.`,start_ms:1238880,video_id:`qJ-8R9CSMsw`,video_title:`Cosmic Queries ‚Äì Wormhole Universe, Black Holes, & Simulations with Neil deGrasse Tyson`},{channel_name:`Harvard University`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:2850720,phrase:`OK, so entropy, conditional <b>entropy</b>, mutual information.`,start_ms:2845920,video_id:`dtEvIva4N2s`,video_title:`Algorithms for Big Data (COMPSCI 229r), Lecture 9`},{channel_name:`Robinson Erhardt`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:2146e3,phrase:`It would be extraordinarily strange if they had an <b>entropy</b>, but they did not store information.`,start_ms:2139440,video_id:`XHpsRNEJ7ZU`,video_title:`Raphael Bousso: The Black Hole Paradox, Quantum Gravity, and the Holographic Principle`},{channel_name:`Lex Fridman`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:4132960,phrase:`Why was gravity in this very low <b>entropy</b> state, very highly organised state.`,start_ms:4125720,video_id:`orMtwOz6Db0`,video_title:`Roger Penrose: Physics of Consciousness and the Infinite Universe | Lex Fridman Podcast #85`},{channel_name:`Lex Fridman`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:5414960,phrase:`And then they can, they just, they contribute noise and <b>entropy</b> into everything and they bloat stuff.`,start_ms:5408680,video_id:`cdiD-9MMpb0`,video_title:`Andrej Karpathy: Tesla AI, Self-Driving, Optimus, Aliens, and AGI | Lex Fridman Podcast #333`},{channel_name:`Chris Williamson`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:1949680,phrase:`But you draw a line between <b>entropy</b> and thought and this impermanence of thought as well.`,start_ms:1944160,video_id:`k_nraMULtBA`,video_title:`Brian Greene - The Mind-bending Physics Of Eternity | Modern Wisdom Podcast 308`},{channel_name:`Tim Ferriss`,concept_id:`ct:ct7eZdGyUKSQcSZBXHfnZentropy`,end_ms:5414240,phrase:`It can go too far and leads to magical thinking, which is also on the too much <b>entropy</b> side, but it also can help break people out of those habits.`,start_ms:5404160,video_id:`cbBXplrwbyQ`,video_title:`Michael Pollan Interview | The Tim Ferriss Show (Podcast)`}];document.readyState===`loading`?document.addEventListener(`DOMContentLoaded`,function(){new YouTubeExamplesPlayer(`youtube-player-entropy|cluster.135947`,examplesData).initialize()}):new YouTubeExamplesPlayer(`youtube-player-entropy|cluster.135947`,examplesData).initialize()})();</script></div><div class=related><h3>Related Terms</h3><ul><li><span class=muted>synonym</span>: <a href=../headwords/information.html>information</a> - <em>facts provided or learned about something</em></ul></div><div><h3>Senses in This Cluster</h3><ul><li><code>entropy|oewn-05098731-n</code> - uncertainty measure in information theory</ul></div><div><h3>Surface Forms</h3><ul><li><a href=../surfaceforms/entropy.html><code>entropy</code></a> - singular (base)<li><a href=../surfaceforms/entropies.html><code>entropies</code></a> - plural</ul></div><div><h3>V2 Concept Mappings</h3><p class=muted>This cluster maps to the following V2 concept IDs:<ul><li><code>ct:csuhvpwSw7mZrWqbTL1mCentropy</code></ul></div><div><h3>Frequency Details</h3><div class=frequency-details><table style=border-collapse:collapse;width:100%;margin-bottom:1rem><thead><tr style="border-bottom:2px solid #ddd"><th style=text-align:left;padding:8px>Corpus<th style=text-align:right;padding:8px>Occurrences<th style=text-align:right;padding:8px>Total Words<th style=text-align:right;padding:8px>Per Million<tbody><tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>mag</code><td style=text-align:right;padding:8px>15<td style=text-align:right;padding:8px>67,696,835<td style=text-align:right;padding:8px>0.22<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>web</code><td style=text-align:right;padding:8px>56<td style=text-align:right;padding:8px>98,085,816<td style=text-align:right;padding:8px>0.57<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>spok</code><td style=text-align:right;padding:8px>0<td style=text-align:right;padding:8px>98,046,614<td style=text-align:right;padding:8px>0.00<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>blog</code><td style=text-align:right;padding:8px>31<td style=text-align:right;padding:8px>96,701,264<td style=text-align:right;padding:8px>0.32<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>news</code><td style=text-align:right;padding:8px>0<td style=text-align:right;padding:8px>95,124,381<td style=text-align:right;padding:8px>0.00<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>fic</code><td style=text-align:right;padding:8px>5<td style=text-align:right;padding:8px>93,827,527<td style=text-align:right;padding:8px>0.05<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>acad</code><td style=text-align:right;padding:8px>19<td style=text-align:right;padding:8px>44,705,772<td style=text-align:right;padding:8px>0.43<tr style="border-bottom:1px solid #eee"><td style=padding:8px><code>tvm</code><td style=text-align:right;padding:8px>1<td style=text-align:right;padding:8px>91,324,286<td style=text-align:right;padding:8px>0.01<tr style="border-top:2px solid #ddd;font-weight:700"><td style=padding:8px>Mean<td style=text-align:right;padding:8px>‚Äî<td style=text-align:right;padding:8px>‚Äî<td style=text-align:right;padding:8px>0.20<tr style=font-weight:700><td style=padding:8px>Median<td style=text-align:right;padding:8px>‚Äî<td style=text-align:right;padding:8px>‚Äî<td style=text-align:right;padding:8px>0.14</table><div style=background-color:#f8f9fa;border-radius:4px;margin-top:1rem;padding:12px><div style=margin-bottom:8px;font-weight:700>UI Frequency (0.06 per million)</div><div style=margin-left:1rem><div><strong>Per year:</strong> 1.1 occurrences</div><div><strong>Per month:</strong> 0.1 occurrences</div><div><strong>Per week:</strong> 0.0 occurrences</div></div></div></div></div><div><h3>Origin Information</h3><div class=origin-tree><div class=origin-card><div class="origin-header origin-transform"><span class=origin-icon>üîÑ</span><span class=origin-type>Transform</span></div><div class=origin-body><div class=origin-field><span class=origin-label>Step:</span><span class=origin-value>regional_clusters_merged</span></div><div class=origin-field><span class=origin-label>Result ID:</span><code class=origin-code>entropy|cluster.135947</code></div><div class="origin-field origin-description">In information theory, a numerical measure of the uncertainty or unpredictability of the possible outcomes in a set of messages.</div></div><div class=origin-arrow>‚Üì</div><div class="origin-card origin-nested-card"><div class="origin-header origin-transform"><span class=origin-icon>üîÑ</span><span class=origin-type>Transform</span></div><div class=origin-body><div class=origin-field><span class=origin-label>Step:</span><span class=origin-value>clustered_senses</span></div><div class=origin-field><span class=origin-label>Result ID:</span><code class=origin-code>entropy|cluster.135947</code></div><div class="origin-field origin-description">In information theory, a numerical measure of the uncertainty or unpredictability of the possible outcomes in a set of messages.</div></div><div class=origin-arrow>‚Üì</div><div class="origin-card origin-nested-card"><div class="origin-header origin-source"><span class=origin-icon>üìö</span><span class=origin-type>Source</span></div><div class=origin-body><div class=origin-field><span class=origin-label>Type:</span><span class=origin-value>dict_v42_full</span></div><div class=origin-field><span class=origin-label>ID:</span><code class=origin-code>entropy|oewn-05098731-n</code></div><div class=origin-field><span class=origin-label>Headword:</span><span class=origin-value>entropy</span></div><div class="origin-field origin-description">In information theory, a numerical measure of the uncertainty or unpredictability of the possible outcomes in a set of messages.</div></div></div></div></div></div></div><div class=muted style=margin-top:10px;font-size:.9em><div><span>Cluster ID:</span><code>entropy|cluster.135947</code></div><div><span>V2 Concepts:</span><code>ct:csuhvpwSw7mZrWqbTL1mCentropy</code></div><div><span>V3 Concept ID:</span><code>ct:ct7eZdGyUKSQcSZBXHfnZentropy</code></div></div></section></main></div>