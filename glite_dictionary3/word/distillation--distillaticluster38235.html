
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>
                distillation – cluster
            </title>
            <link rel="stylesheet" href="../style.css">
        </head>
        <body>
            <div class="container">
                <header>
                    <p class='small'><a href='distillation.html'>&larr; Back to distillation</a></p>
                    <h1>distillation</h1>
                    <div class='part-of-speech non-italic'>noun</div>
                    <div class='headword-subtitle'>machine-learning model compression method</div>
                </header>
                <main>
                    <section class="concept-entry concept-first">
                        <div class='headword-pronunciation'>Pronunciation: / ˌdɪstəˈleɪʃən / (AmE), / ˌdɪstəˈleɪʃən / (BrE)</div>
                        <div class="definition">(in machine learning) a method of transferring knowledge from a large, high-capacity model to a smaller model by training the smaller model to mimic the larger model's outputs or behaviour.</div>
                        <div class="examples"><h3>Examples</h3><blockquote>model <b>distillation</b></blockquote><blockquote>Researchers used <b>distillation</b> to compress a large language model into a smaller, faster version.</blockquote><blockquote>By applying <b>distillation</b>, engineers taught the compact model to mimic outputs from a high-capacity teacher network.</blockquote><blockquote>Production systems often rely on <b>distillation</b> for deploying models that balance accuracy and inference speed in mobile apps.</blockquote></div>
                        <div class='related'><h3>Related Terms</h3><ul><li><span class='muted'>synonym</span> – knowledge distillation: <em>the technique of training a smaller 'student' model to imitate a larger 'teacher' model's outputs</em></li><li><span class='muted'>synonym</span> – model compression: <em>methods for reducing a machine-learning model's size and computation for deployment</em></li></ul></div>
                        <div><h3>Senses in This Cluster</h3><ul><li><code>distillati|augmented.1234</code> – machine-learning model compression method</li></ul></div>
                    </section>
                </main>
                <footer><p class="muted">Cluster assets derived from dictionary data</p></footer>
            </div>
        </body>
        </html>
    