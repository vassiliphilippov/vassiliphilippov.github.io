
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>
                distil – cluster
            </title>
            <link rel="stylesheet" href="../style.css">
        </head>
        <body>
            <div class="container">
                <header>
                    <p class='small'><a href='distil.html'>&larr; Back to distil</a></p>
                    <h1>to distil</h1>
                    <div class='part-of-speech non-italic'>verb</div>
                    <div class='headword-subtitle'>transfer knowledge between ML models</div>
                </header>
                <main>
                    <section class="concept-entry concept-first">
                        <div class='headword-pronunciation'>Pronunciation: / dɪˈstɪl / (AmE), / dɪˈstɪl / (BrE)</div>
                        <div class="definition">(in machine learning) to transfer learned knowledge or behaviour from a larger, typically slower model (the "teacher") to a smaller, faster model (the "student"), usually by training the smaller model to match the larger model's outputs or internal representations.</div>
                        <div class="examples"><h3>Examples</h3><blockquote>to <b>distil</b> model knowledge</blockquote><blockquote>Researchers <b>distil</b> the teacher model into a compact student network for deployment on mobile devices.</blockquote><blockquote>To improve speed, the team <b>distilled</b> the large language model into a smaller, efficient variant.</blockquote><blockquote>Their paper shows how to <b>distil</b> knowledge using soft targets and intermediate feature matching techniques.</blockquote></div>
                        <div class='related'><h3>Related Terms</h3><ul><li><span class='muted'>synonym</span> – knowledge distillation: <em>the machine-learning technique of transferring knowledge from a large model to a smaller one</em></li><li><span class='muted'>synonym</span> – model compression: <em>methods for producing smaller, more efficient machine-learning models from larger ones</em></li></ul></div>
                        <div><h3>Senses in This Cluster</h3><ul><li><code>distil|augmented.1255</code> – transfer knowledge between ML models</li></ul></div>
                    </section>
                </main>
                <footer><p class="muted">Cluster assets derived from dictionary data</p></footer>
            </div>
        </body>
        </html>
    