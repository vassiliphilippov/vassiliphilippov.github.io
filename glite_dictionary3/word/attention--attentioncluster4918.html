
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>
                attention – cluster
            </title>
            <link rel="stylesheet" href="../style.css">
        </head>
        <body>
            <div class="container">
                <header>
                    <p class='small'><a href='attention.html'>&larr; Back to attention</a></p>
                    <h1>attention</h1>
                    <div class='part-of-speech non-italic'>noun</div>
                    <div class='headword-subtitle'>ML mechanism that weights input elements</div>
                </header>
                <main>
                    <section class="concept-entry concept-first">
                        <div class='headword-pronunciation'>Pronunciation: / 59c8t5bn8359n / (AmE), / 59c8t5bn83(59)n / (BrE)</div>
                        <div class="definition">(in machine learning) a mechanism in neural networks that assigns dynamic weights to different parts of the input so the model focuses on the most relevant information.</div>
                        <div class="examples"><h3>Examples</h3><blockquote><b>attention</b> mechanism in AI</blockquote><blockquote>The transformer model uses an <b>attention</b> mechanism to weigh each word in the input sequence.</blockquote><blockquote>Researchers improved translation quality by adding multi-head <b>attention</b> to the neural network architecture.</blockquote><blockquote>By visualising <b>attention</b> weights, they could see which tokens the model focused on during prediction.</blockquote></div>
                        <div class='related'><h3>Related Terms</h3><ul><li><span class='muted'>synonym</span> – attention mechanism: <em>a neural network component that computes weights over input elements to emphasize relevant information</em></li><li><span class='muted'>synonym</span> – self-attention: <em>a form of attention where a sequence's elements attend to other elements in the same sequence</em></li></ul></div>
                        <div><h3>Senses in This Cluster</h3><ul><li><code>attention|augmented.556</code> – ML mechanism that weights input elements</li></ul></div>
                    </section>
                </main>
                <footer><p class="muted">Cluster assets derived from dictionary data</p></footer>
            </div>
        </body>
        </html>
    