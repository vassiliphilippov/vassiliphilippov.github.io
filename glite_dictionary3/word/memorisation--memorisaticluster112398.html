
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>
                memorisation – cluster
            </title>
            <link rel="stylesheet" href="../style.css">
        </head>
        <body>
            <div class="container">
                <header>
                    <p class='small'><a href='memorisation.html'>&larr; Back to memorisation</a></p>
                    <h1>memorisation</h1>
                    <div class='part-of-speech non-italic'>noun</div>
                    <div class='headword-subtitle'>model storing training data</div>
                </header>
                <main>
                    <section class="concept-entry concept-first">
                        <div class='headword-pronunciation'>Pronunciation: / ccm5bm.59.ra6ac8ze6a.8359n / (AmE), / ccm5bm.59r.a6ac8ze6a.8359n / (BrE)</div>
                        <div class="definition">(in machine learning) the tendency of a model to reproduce or store exact training examples or data fragments instead of learning general patterns, often causing poor performance on new data or data leakage.</div>
                        <div class="examples"><h3>Examples</h3><blockquote><b>memorisation</b> in models</blockquote><blockquote>Researchers warned that the model's <b>memorisation</b> of training examples could leak private user information.</blockquote><blockquote>To avoid <b>memorisation</b>, the team added dropout and data augmentation during model training.</blockquote><blockquote>Journal reviewers criticised the paper for evidence of <b>memorisation</b> rather than genuine generalisation to new data.</blockquote></div>
                        <div class='related'><h3>Related Terms</h3><ul><li><span class='muted'>antonym</span> – <a href='../word/generalisation.html'>generalisation</a>: <em>the ability of a model to perform well on unseen data; opposite of memorisation in ML contexts</em></li><li><span class='muted'>synonym</span> – overfitting: <em>when a model learns noise or details of the training data and performs poorly on new data</em></li></ul></div>
                        <div><h3>Senses in This Cluster</h3><ul><li><code>memorisati|augmented.2178</code> – model storing training data</li></ul></div>
                    </section>
                </main>
                <footer><p class="muted">Cluster assets derived from dictionary data</p></footer>
            </div>
        </body>
        </html>
    