
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>To distil complex ideas - Dictionary</title>
        <link rel="stylesheet" href="../style.css">
    </head>
    <body>
        <div class="container">
            <header>
                <h1>to distil complex ideas</h1>
            </header>
            <main>
                
        <section class="concept-entry new-sense">
            <h2>1. <span class="part-of-speech">VERB</span> <span class="new-badge">NEW</span></h2>
            
            <div class="headword-pronunciation">
                / dɪˈstɪl / (AmE), / dɪˈstɪl / (BrE)
            </div>
            
            <div class="definition">(in machine learning) to transfer learned knowledge or behaviour from a larger, typically slower model (the "teacher") to a smaller, faster model (the "student"), usually by training the smaller model to match the larger model's outputs or internal representations.</div>
            <div class="short-description">transfer knowledge between ML models</div>
            
            <div class="examples"><h3>Examples</h3><blockquote>Researchers <b>distil</b> the teacher model into a compact student network for deployment on mobile devices.</blockquote><blockquote>To improve speed, the team <b>distilled</b> the large language model into a smaller, efficient variant.</blockquote><blockquote>Their paper shows how to <b>distil</b> knowledge using soft targets and intermediate feature matching techniques.</blockquote></div>
            <div class="examples"><h3>Synset Examples</h3><blockquote>Researchers distil the teacher model into a compact student network for deployment on mobile devices.</blockquote><blockquote>To improve speed, the team distilled the large language model into a smaller, efficient variant.</blockquote><blockquote>Their paper shows how to distil knowledge using soft targets and intermediate feature matching techniques.</blockquote></div>
            <div class="linked-concepts"><h3>Related Terms</h3><ul>
                <li>
                    <span class="linked-kind">Synonym:</span>
                    knowledge distillation
                    - <em>the machine-learning technique of transferring knowledge from a large model to a smaller one</em>
                </li>
                
                <li>
                    <span class="linked-kind">Synonym:</span>
                    model compression
                    - <em>methods for producing smaller, more efficient machine-learning models from larger ones</em>
                </li>
                </ul></div>
            
            <div class="etymology">
                <h3>Etymology</h3>
                <p class="etymology-body">From Latin 'destillare' (to trickle down); figuratively used to mean extract the essence.</p>
            </div>
            
            
            <div class="meta">
                <div class="meta-item"><strong>Source:</strong> Unknown</div>
                <div class="meta-item"><strong>WordNet ID:</strong> n/a</div>
                <div class="meta-item"><strong>Synset POS:</strong> </div>
            </div>
        
        </section>
        
            </main>
            <footer><p><a href="../index.html">Back to Home</a></p></footer>
        </div>
    </body>
    </html>
    